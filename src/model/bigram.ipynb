{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjzW2j49z7tA",
        "outputId": "03dd7a8b-8743-476e-ce5e-eeef80ce565f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-06-10 15:44:27--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-06-10 15:44:27 (27.4 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# start with the datatset to train on, called \"tinyshakspeare\" dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3Xr2d_o0D4D",
        "outputId": "40a2704a-1b7f-4f3b-e9e6-47d513c02fb3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-7ee1c09ea9d0>:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  data = torch.tensor(encode(text), dtype=torch.long)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyper parameters\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "lr = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ---------------\n",
        "\n",
        "# dataset\n",
        "dataset_name = 'tinyshakspeare'\n",
        "\n",
        "# -------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open(f'input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# all unique characters in dataset\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# mapping from characters to integers\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: torch.LongTensor([stoi[ch] for ch in s])\n",
        "decode = lambda x: ''.join([itos[i] for i in x])\n",
        "\n",
        "# train and test split\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "#data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        X, Y = get_batch(split)\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class BigramModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding = nn.Embedding(block_size, n_embd)\n",
        "        # self.sa_head = Head(n_embd)\n",
        "        # i.e. 4 heads of 8-dimensional self-attention\n",
        "        # the total dimension of the self-attention is n_embd = 32\n",
        "        # self.sa_heads = MultiHeadAttention(4, n_embd // 4)\n",
        "        # self.ffwd = FeedForward(n_embd)\n",
        "\n",
        "        # self.blocks = nn.Sequential(\n",
        "        #     Block(n_embd, n_head=4),\n",
        "        #     Block(n_embd, n_head=4),\n",
        "        #     Block(n_embd, n_head=4),\n",
        "        # )\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx and target are both (B, T) tensor of integers\n",
        "        B, T = idx.shape\n",
        "        tok_embd = self.token_embedding(idx) # (B, T, C)\n",
        "        pos_embd = self.position_embedding(torch.arange(T, device=device)) # (T, C)\n",
        "        x = tok_embd + pos_embd\n",
        "\n",
        "        # self-attention\n",
        "        # x = self.sa_head(x) # (B, T, C)\n",
        "        # x = self.sa_heads(x) # (B, T, C)\n",
        "        # x = self.ffwd(x)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x) # (B, T, V)\n",
        "\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(-1)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_token):\n",
        "        #idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_token):\n",
        "            # crop idx to the lst block_size tokens\n",
        "            # since the embedding table only has block_size rows\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get prediction\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last timestamp\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "            # append to the context\n",
        "            idx = torch.cat([idx, next_token], dim=1)\n",
        "        return idx\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple feed-forward module \"\"\"\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd), # 4* n_embd in transformer paper\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd), # projection\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        K = self.key(x) # (B, T, H)\n",
        "        Q = self.query(x) # (B, T, H)\n",
        "        # compute the attention scores(\"affinity\")\n",
        "        wei = (Q @ K.transpose(-2, -1)) / (C ** 0.5)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # weighted aggregation\n",
        "        V = self.value(x) # (B, T, H)\n",
        "        out = wei @ V   # (B, T, T) @ (B, T, H) = (B, T, H)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel. \"\"\"\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation.\"\"\"\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa_heads = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        # Add layer norm before going to self-attention and feed forward\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # trick 1: residual connection\n",
        "        x = x + self.sa_heads(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8LAkY5qE0dZr",
        "outputId": "5f664d33-4f76-41e5-9989-755703687ef1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYmvlDwy0GI6",
        "outputId": "98d350c2-fa72-4388-ba03-f794d510936a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iter 0 Train loss 4.28 Val loss 4.28\n",
            "Iter 500 Train loss 2.00 Val loss 2.08\n",
            "Iter 1000 Train loss 1.60 Val loss 1.78\n",
            "Iter 1500 Train loss 1.44 Val loss 1.65\n",
            "Iter 2000 Train loss 1.34 Val loss 1.58\n",
            "Iter 2500 Train loss 1.28 Val loss 1.53\n",
            "Iter 3000 Train loss 1.23 Val loss 1.51\n",
            "Iter 3500 Train loss 1.18 Val loss 1.49\n",
            "Iter 4000 Train loss 1.15 Val loss 1.48\n",
            "Iter 4500 Train loss 1.11 Val loss 1.47\n",
            "\n",
            "But with prison: I will stead find to him.\n",
            "Whympt, Clarence, his judge friends, let withal:\n",
            "Where should betide, true in Rome, my stand's love?\n",
            "\n",
            "QUEEN MARGARET:\n",
            "Ah, what you may make less, sleep leave mine own,\n",
            "And nothing lack of the time over your owes?\n",
            "\n",
            "QUEEN MARGARET:\n",
            "Anon Herefort, Honour! to fine till I clead.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Strong of that, if you do your heart?\n",
            "You love then speak 'H leave power, well't chase you come.\n",
            "\n",
            "GLOUCESTER:\n",
            "Tut, that certain me not of the first:\n",
            "Among strong nature impossion work than cont\n",
            "Engoing thy face, butchery and necessities\n",
            "The riests of consul.\n",
            "\n",
            "TYREL:\n",
            "My liege, my kind-such cloyary: Tyrrel, live him,\n",
            "Laids aways thy dark! that widings, lady's blood\n",
            "The lane of dash.\n",
            "\n",
            "SICINIUS:\n",
            "Good morrow!\n",
            "We govern'd thee, come,\n",
            "A fear to dry thee on me; what thou, my lord,\n",
            "We had beard's confass off wife, or well a fire-live.\n",
            "So outor his resemble cold\n",
            "Upon puis moutarst your paper.\n",
            "\n",
            "MARCIUS:\n",
            "That thou hast a faw, and see you mean,\n",
            "Where he come to me no c\n"
          ]
        }
      ],
      "source": [
        "model = BigramModel()\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f'Iter {iter} Train loss {losses[\"train\"]:.2f} Val loss {losses[\"val\"]:.2f}')\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    #eval the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
        "output = decode(model.generate(context, 1000)[0].tolist())\n",
        "with open(f'output.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(output)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "c263s4HRBMaL"
      },
      "outputs": [],
      "source": [
        "# save the model dict\n",
        "torch.save(model.state_dict(), 'bigram_model.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
